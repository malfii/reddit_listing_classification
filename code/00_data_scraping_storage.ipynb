{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scraping and storing\n",
    "This file is used to scrape data from reddit, put it in the right format, and save it in our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os # to check for file existance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_listings(res, nskip = 0):\n",
    "    \"\"\"\n",
    "    This funciton accept the get object from an api request in reddit and extracts necessary information in the form of dataframe\n",
    "    Args:\n",
    "        res (request model response): request.get object from reddit \n",
    "        nskip (int): number of records to skip in each request (not necessary info), default it 0 for reddit\n",
    "    Return:\n",
    "        dataframe: a datafram with the following columns generated from the request ['text', 'title', 'listingid', 'created', 'url', 'media']\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    records = []\n",
    "    for listing in res.json()['data']['children']:\n",
    "        \n",
    "        # only use the information you need from the each listing  \n",
    "        text = listing['data']['selftext']\n",
    "        title = listing['data']['title']\n",
    "        listingid = listing['data']['id']\n",
    "        created = listing['data']['created']\n",
    "        url = listing['data']['url']\n",
    "        #media_embed = listing['data']['media_embed']\n",
    "        media = listing['data']['media']\n",
    "\n",
    "        records.append([text, title, listingid, created, url, media])\n",
    "\n",
    "    return pd.DataFrame(records, columns=['text', 'title', 'listingid', 'created', 'url', 'media'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data scraping scripts\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read person information\n",
    "# Using readlines()\n",
    "file1 = open('./../personal/my_data.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    " \n",
    "# Strips the newline character\n",
    "personal = []\n",
    "for line in Lines:\n",
    "    personal.append(line.split('=')[1].replace('\\n', ''))\n",
    "\n",
    "client_id = personal[0] #alphanumeric string provided under \"personal use script\"\n",
    "client_secret = personal[1] #alphanumeric string provided as \"secret\"\n",
    "user_agent = personal[2] #the name of your application\n",
    "username =  personal[3] #your reddit username\n",
    "password =  personal[4] #your reddit password\n",
    "\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use basic authentication framework\n",
    "auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': username,\n",
    "    'password': password\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#create an informative header for your application\n",
    "headers = {'User-Agent': 'massiproj3/0.0.1'}\n",
    "\n",
    "res = requests.post(\n",
    "    'https://www.reddit.com/api/v1/access_token',\n",
    "    auth=auth,\n",
    "    data=data,\n",
    "    headers=headers)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve access token\n",
    "token = res.json()['access_token']\n",
    "# add access token to the header file\n",
    "headers['Authorization'] = f'bearer {token}'\n",
    "\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data extraction for subreddit offmychest as related .csv file exists in the folder\n"
     ]
    }
   ],
   "source": [
    "# This section collects the 1000 available records for a given subreddit\n",
    "# Note that this block of code should just run one time to create the initial database.\n",
    "# after that, we will check to see if anything has been added to what we have or not.\n",
    "\n",
    "subreddit = 'offmychest'\n",
    "path = './../dataset/'+ subreddit+'.csv'\n",
    "count = 0\n",
    "if not os.path.isfile(path):    # continue only if the file does not exist\n",
    "    print(f'extracting information for subreddit {subreddit}')\n",
    "    # read the first 100\n",
    "    url = 'https://oauth.reddit.com/r/'+subreddit+'/new'\n",
    "    params = {\n",
    "        'limit': 100\n",
    "    # 'after': <-- will be important for getting the 'next' posts\n",
    "    }\n",
    "    res = requests.get(url, \n",
    "                       headers=headers,\n",
    "                       params=params)\n",
    "    count += 1\n",
    "    print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "    bottom_of_listing = res.json()['data']['after']\n",
    "    df = extract_listings(res, nskip = 0)\n",
    "    # continue reading the rest of 900 in batches of 100\n",
    "    for i in range(9):\n",
    "        params = {'limit': 100, \n",
    "                  'after': bottom_of_listing   # this will read 100 records under the first one\n",
    "                  }\n",
    "        res = requests.get(url, \n",
    "                        headers=headers,\n",
    "                        params=params) \n",
    "        count += 1\n",
    "        print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "        bottom_of_listing = res.json()['data']['after']\n",
    "        df = pd.concat([df, extract_listings(res, nskip = 0)], axis=0) \n",
    "    \n",
    "    print(f'df shape before dropping the repeats {df.shape}')\n",
    "    df = df.drop_duplicates(subset='listingid', keep='first')\n",
    "    print(f'df shape after dropping the repeats {df.shape}')\n",
    "    df.to_csv(path, index=False)\n",
    "else:\n",
    "    print(f'no data extraction for subreddit {subreddit} as related .csv file exists in the folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data extraction for subreddit legaladvice as related .csv file exists in the folder\n"
     ]
    }
   ],
   "source": [
    "# This section collects the 1000 available records for a given subreddit\n",
    "# Note that this block of code should just run one time to create the initial database.\n",
    "# after that, we will check to see if anything has been added to what we have or not.\n",
    "\n",
    "subreddit = 'legaladvice'\n",
    "path = './../dataset/'+ subreddit+'.csv'\n",
    "count = 0\n",
    "if not os.path.isfile(path):    # continue only if the file does not exist\n",
    "    print(f'extracting information for subreddit {subreddit}')\n",
    "    # read the first 100\n",
    "    url = 'https://oauth.reddit.com/r/'+subreddit+'/new'\n",
    "    params = {\n",
    "        'limit': 100\n",
    "    # 'after': <-- will be important for getting the 'next' posts\n",
    "    }\n",
    "    res = requests.get(url, \n",
    "                       headers=headers,\n",
    "                       params=params)\n",
    "    count += 1\n",
    "    print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "    bottom_of_listing = res.json()['data']['after']\n",
    "    df = extract_listings(res, nskip = 0)\n",
    "    # continue reading the rest of 900 in batches of 100\n",
    "    for i in range(9):\n",
    "        params = {'limit': 100, \n",
    "                  'after': bottom_of_listing   # this will read 100 records under the first one\n",
    "                  }\n",
    "        res = requests.get(url, \n",
    "                        headers=headers,\n",
    "                        params=params) \n",
    "        count += 1\n",
    "        print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "        bottom_of_listing = res.json()['data']['after']\n",
    "        df = pd.concat([df, extract_listings(res, nskip = 0)], axis=0) \n",
    "    \n",
    "    print(f'df shape before dropping the repeats {df.shape}')\n",
    "    df = df.drop_duplicates(subset='listingid', keep='first')\n",
    "    print(f'df shape after dropping the repeats {df.shape}')\n",
    "    df.to_csv(path, index=False)\n",
    "else:\n",
    "    print(f'no data extraction for subreddit {subreddit} as related .csv file exists in the folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data extraction for subreddit politics as related .csv file exists in the folder\n"
     ]
    }
   ],
   "source": [
    "# This section collects the 1000 available records for a given subreddit\n",
    "# Note that this block of code should just run one time to create the initial database.\n",
    "# after that, we will check to see if anything has been added to what we have or not.\n",
    "\n",
    "subreddit = 'politics'\n",
    "path = './../dataset/'+ subreddit+'.csv'\n",
    "count = 0\n",
    "if not os.path.isfile(path):    # continue only if the file does not exist\n",
    "    print(f'extracting information for subreddit {subreddit}')\n",
    "    # read the first 100\n",
    "    url = 'https://oauth.reddit.com/r/'+subreddit+'/new'\n",
    "    params = {\n",
    "        'limit': 100\n",
    "    # 'after': <-- will be important for getting the 'next' posts\n",
    "    }\n",
    "    res = requests.get(url, \n",
    "                       headers=headers,\n",
    "                       params=params)\n",
    "    count += 1\n",
    "    print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "    bottom_of_listing = res.json()['data']['after']\n",
    "    df = extract_listings(res, nskip = 0)\n",
    "    # continue reading the rest of 900 in batches of 100\n",
    "    for i in range(9):\n",
    "        params = {'limit': 100, \n",
    "                  'after': bottom_of_listing   # this will read 100 records under the first one\n",
    "                  }\n",
    "        res = requests.get(url, \n",
    "                        headers=headers,\n",
    "                        params=params) \n",
    "        count += 1\n",
    "        print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "        bottom_of_listing = res.json()['data']['after']\n",
    "        df = pd.concat([df, extract_listings(res, nskip = 0)], axis=0) \n",
    "    \n",
    "    print(f'df shape before dropping the repeats {df.shape}')\n",
    "    df = df.drop_duplicates(subset='listingid', keep='first')\n",
    "    print(f'df shape after dropping the repeats {df.shape}')\n",
    "    df.to_csv(path, index=False)\n",
    "else:\n",
    "    print(f'no data extraction for subreddit {subreddit} as related .csv file exists in the folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data extraction for subreddit dating_advice as related .csv file exists in the folder\n"
     ]
    }
   ],
   "source": [
    "# This section collects the 1000 available records for a given subreddit\n",
    "# Note that this block of code should just run one time to create the initial database.\n",
    "# after that, we will check to see if anything has been added to what we have or not.\n",
    "\n",
    "subreddit = 'dating_advice'\n",
    "path = './../dataset/'+ subreddit+'.csv'\n",
    "count = 0\n",
    "if not os.path.isfile(path):    # continue only if the file does not exist\n",
    "    print(f'extracting information for subreddit {subreddit}')\n",
    "    # read the first 100\n",
    "    url = 'https://oauth.reddit.com/r/'+subreddit+'/new'\n",
    "    params = {\n",
    "        'limit': 100\n",
    "    # 'after': <-- will be important for getting the 'next' posts\n",
    "    }\n",
    "    res = requests.get(url, \n",
    "                       headers=headers,\n",
    "                       params=params)\n",
    "    count += 1\n",
    "    print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "    bottom_of_listing = res.json()['data']['after']\n",
    "    df = extract_listings(res, nskip = 0)\n",
    "    # continue reading the rest of 900 in batches of 100\n",
    "    for i in range(9):\n",
    "        params = {'limit': 100, \n",
    "                  'after': bottom_of_listing   # this will read 100 records under the first one\n",
    "                  }\n",
    "        res = requests.get(url, \n",
    "                        headers=headers,\n",
    "                        params=params) \n",
    "        count += 1\n",
    "        print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "        bottom_of_listing = res.json()['data']['after']\n",
    "        df = pd.concat([df, extract_listings(res, nskip = 0)], axis=0) \n",
    "    \n",
    "    print(f'df shape before dropping the repeats {df.shape}')\n",
    "    df = df.drop_duplicates(subset='listingid', keep='first')\n",
    "    print(f'df shape after dropping the repeats {df.shape}')\n",
    "    df.to_csv(path, index=False)\n",
    "else:\n",
    "    print(f'no data extraction for subreddit {subreddit} as related .csv file exists in the folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data extraction for subreddit investing as related .csv file exists in the folder\n"
     ]
    }
   ],
   "source": [
    "# This section collects the 1000 available records for a given subreddit\n",
    "# Note that this block of code should just run one time to create the initial database.\n",
    "# after that, we will check to see if anything has been added to what we have or not.\n",
    "\n",
    "subreddit = 'investing'\n",
    "path = './../dataset/'+ subreddit+'.csv'\n",
    "count = 0\n",
    "if not os.path.isfile(path):    # continue only if the file does not exist\n",
    "    print(f'extracting information for subreddit {subreddit}')\n",
    "    # read the first 100\n",
    "    url = 'https://oauth.reddit.com/r/'+subreddit+'/new'\n",
    "    params = {\n",
    "        'limit': 100\n",
    "    # 'after': <-- will be important for getting the 'next' posts\n",
    "    }\n",
    "    res = requests.get(url, \n",
    "                       headers=headers,\n",
    "                       params=params)\n",
    "    count += 1\n",
    "    print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "    bottom_of_listing = res.json()['data']['after']\n",
    "    df = extract_listings(res, nskip = 0)\n",
    "    # continue reading the rest of 900 in batches of 100\n",
    "    for i in range(9):\n",
    "        params = {'limit': 100, \n",
    "                  'after': bottom_of_listing   # this will read 100 records under the first one\n",
    "                  }\n",
    "        res = requests.get(url, \n",
    "                        headers=headers,\n",
    "                        params=params) \n",
    "        count += 1\n",
    "        print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "        bottom_of_listing = res.json()['data']['after']\n",
    "        df = pd.concat([df, extract_listings(res, nskip = 0)], axis=0) \n",
    "    \n",
    "    print(f'df shape before dropping the repeats {df.shape}')\n",
    "    df = df.drop_duplicates(subset='listingid', keep='first')\n",
    "    print(f'df shape after dropping the repeats {df.shape}')\n",
    "    df.to_csv(path, index=False)\n",
    "else:\n",
    "    print(f'no data extraction for subreddit {subreddit} as related .csv file exists in the folder')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block of the code, we will take a list of the subreddits we are interested in. If there is no previous information collected from those subreddits, we will create a new .csv file and start collecting the data. If we already have some data from that subreddit, it will just append the new data to what we already have. It is recommended that we run this block of the code every 2-3 days so that new information will be fetched and recorded. \n",
    "Subreddits with similarities (based on users posting in subreddits, source: https://anvaka.github.io/sayit/?query=dating_advice):\n",
    "- offmychest, askreddit, nostupidquestions, dating_advice, relationship_advice\n",
    "- legaladvice, nostupidquestions\n",
    "- dating_advice, relationship_advice \n",
    "- investing, wallstreetbets (just titles, no text as a lot of pics exist)\n",
    "- politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++> no initial data extraction for subreddit offmychest as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit trueoffmychest as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit askreddit as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit nostupidquestions as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit dating_advice as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit relationship_advice as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit legaladvice as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit politics as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit investing as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit wallstreetbets as related .csv file exists in the folder\n",
      "==> extracting daily information for subreddit offmychest\n",
      "request.get() status for the 1\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 2\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 3\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 4\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 5\"s call for subreddit offmychest is 200\n",
      "a total of 2 new listings was added to offmychest subreddit\n",
      "offmychest.csv file has 1250 records now\n",
      "==> extracting daily information for subreddit trueoffmychest\n",
      "request.get() status for the 1\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 2\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 3\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 4\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 5\"s call for subreddit trueoffmychest is 200\n",
      "a total of 4 new listings was added to trueoffmychest subreddit\n",
      "trueoffmychest.csv file has 999 records now\n",
      "==> extracting daily information for subreddit askreddit\n",
      "request.get() status for the 1\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 2\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 3\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 4\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 5\"s call for subreddit askreddit is 200\n",
      "a total of 17 new listings was added to askreddit subreddit\n",
      "askreddit.csv file has 1028 records now\n",
      "==> extracting daily information for subreddit nostupidquestions\n",
      "request.get() status for the 1\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 2\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 3\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 4\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 5\"s call for subreddit nostupidquestions is 200\n",
      "a total of 10 new listings was added to nostupidquestions subreddit\n",
      "nostupidquestions.csv file has 1013 records now\n",
      "==> extracting daily information for subreddit dating_advice\n",
      "request.get() status for the 1\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 2\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 3\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 4\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 5\"s call for subreddit dating_advice is 200\n",
      "a total of 1 new listings was added to dating_advice subreddit\n",
      "dating_advice.csv file has 1179 records now\n",
      "==> extracting daily information for subreddit relationship_advice\n",
      "request.get() status for the 1\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 2\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 3\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 4\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 5\"s call for subreddit relationship_advice is 200\n",
      "a total of 5 new listings was added to relationship_advice subreddit\n",
      "relationship_advice.csv file has 997 records now\n",
      "==> extracting daily information for subreddit legaladvice\n",
      "request.get() status for the 1\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 2\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 3\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 4\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 5\"s call for subreddit legaladvice is 200\n",
      "a total of 6 new listings was added to legaladvice subreddit\n",
      "legaladvice.csv file has 1312 records now\n",
      "==> extracting daily information for subreddit politics\n",
      "request.get() status for the 1\"s call for subreddit politics is 200\n",
      "request.get() status for the 2\"s call for subreddit politics is 200\n",
      "request.get() status for the 3\"s call for subreddit politics is 200\n",
      "request.get() status for the 4\"s call for subreddit politics is 200\n",
      "request.get() status for the 5\"s call for subreddit politics is 200\n",
      "a total of 1 new listings was added to politics subreddit\n",
      "politics.csv file has 1113 records now\n",
      "==> extracting daily information for subreddit investing\n",
      "request.get() status for the 1\"s call for subreddit investing is 200\n",
      "request.get() status for the 2\"s call for subreddit investing is 200\n",
      "request.get() status for the 3\"s call for subreddit investing is 200\n",
      "request.get() status for the 4\"s call for subreddit investing is 200\n",
      "request.get() status for the 5\"s call for subreddit investing is 200\n",
      "a total of 0 new listings was added to investing subreddit\n",
      "investing.csv file has 889 records now\n",
      "==> extracting daily information for subreddit wallstreetbets\n",
      "request.get() status for the 1\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 2\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 3\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 4\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 5\"s call for subreddit wallstreetbets is 200\n",
      "a total of 1 new listings was added to wallstreetbets subreddit\n",
      "wallstreetbets.csv file has 912 records now\n",
      "total number of records collected from each subreddit so far:\n",
      "[('offmychest', 1250), ('trueoffmychest', 999), ('askreddit', 1028), ('nostupidquestions', 1013), ('dating_advice', 1179), ('relationship_advice', 997), ('legaladvice', 1312), ('politics', 1113), ('investing', 889), ('wallstreetbets', 912)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "subreddits_to_check = ['offmychest', 'trueoffmychest', 'askreddit', 'nostupidquestions', \n",
    "                       'dating_advice', 'relationship_advice',\n",
    "                       'legaladvice', \n",
    "                       'politics', \n",
    "                       'investing', 'wallstreetbets']\n",
    "subredits_total_recs = []\n",
    "req_per_day = 5 # number of inqueries for each subreddit every time we run this script\n",
    "\n",
    "# This section collects the 1000 available records for a given subreddit\n",
    "# Note that this block of code will run just one time to initially retrive 1000 records \n",
    "# for each of the subreddits in the list and create a .csv file for it. \n",
    "# this does not run for subreddits that already have a .csv file associated to them. instead, the \n",
    "# next block of the code will run for those. \n",
    "\n",
    "for subreddit in subreddits_to_check:\n",
    "    path = './../dataset/'+ subreddit+'.csv'\n",
    "    count = 0\n",
    "    if not os.path.isfile(path):    # continue only if the file does not exist\n",
    "        print(f'++> extracting initial information for subreddit {subreddit}')\n",
    "        # read the first 100\n",
    "        url = 'https://oauth.reddit.com/r/'+subreddit+'/new'\n",
    "        params = {\n",
    "            'limit': 100\n",
    "        # 'after': <-- will be important for getting the 'next' posts\n",
    "        }\n",
    "        res = requests.get(url, \n",
    "                        headers=headers,\n",
    "                        params=params)\n",
    "        count += 1\n",
    "        print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "        bottom_of_listing = res.json()['data']['after']\n",
    "        df = extract_listings(res, nskip = 0)\n",
    "        # continue reading the rest of 900 in batches of 100\n",
    "        for i in range(9):\n",
    "            params = {'limit': 100, \n",
    "                    'after': bottom_of_listing   # this will read 100 records under the first one\n",
    "                    }\n",
    "            res = requests.get(url, \n",
    "                            headers=headers,\n",
    "                            params=params) \n",
    "            count += 1\n",
    "            print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "            bottom_of_listing = res.json()['data']['after']\n",
    "            df = pd.concat([df, extract_listings(res, nskip = 0)], axis=0) \n",
    "        \n",
    "        print(f'df shape before dropping the repeats {df.shape}')\n",
    "        df = df.drop_duplicates(subset='listingid', keep='first')\n",
    "        print(f'df shape after dropping the repeats {df.shape}')\n",
    "        df.to_csv(path, index=False)\n",
    "    else:\n",
    "        print(f'++> no initial data extraction for subreddit {subreddit} as related .csv file exists in the folder')\n",
    "\n",
    "\n",
    "# this section of the code is intended to run every2-3 days to retrieve new information for the given list of \n",
    "# the subreddits of interest. the new information will be appended to the list of the .csv files we currently have \n",
    "# and will be saved into the appropriate .csv file\n",
    "        \n",
    "for subreddit in subreddits_to_check:   \n",
    "    path = './../dataset/'+ subreddit+'.csv'\n",
    "    count = 0 \n",
    "    df = pd.read_csv(path) # get the current information we have\n",
    "\n",
    "    print(f'==> extracting daily information for subreddit {subreddit}')\n",
    "    # read the first 100\n",
    "    url = 'https://oauth.reddit.com/r/'+subreddit+'/new'\n",
    "    params = {\n",
    "        'limit': 100\n",
    "    # 'after': <-- will be important for getting the 'next' posts\n",
    "    }\n",
    "    res = requests.get(url, \n",
    "                       headers=headers,\n",
    "                       params=params)\n",
    "    count += 1\n",
    "    print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "    bottom_of_listing = res.json()['data']['after']\n",
    "    temp = extract_listings(res, nskip = 0)\n",
    "    # continue reading the rest of 900 in batches of 100\n",
    "    for i in range(req_per_day-1):\n",
    "        params = {'limit': 100, \n",
    "                  'after': bottom_of_listing   # this will read 100 records under the first one\n",
    "                  }\n",
    "        res = requests.get(url, \n",
    "                        headers=headers,\n",
    "                        params=params) \n",
    "        count += 1\n",
    "        print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "        bottom_of_listing = res.json()['data']['after']\n",
    "        temp = pd.concat([temp, extract_listings(res, nskip = 0)], axis=0) \n",
    "    temp = temp.drop_duplicates(subset='listingid', keep='first')\n",
    "    old_size = df.shape[0]\n",
    "    df = pd.concat([df, temp], axis=0) \n",
    "    df = df.drop_duplicates(subset='listingid', keep='first')\n",
    "\n",
    "    print(f'a total of {df.shape[0]-old_size} new listings was added to {subreddit} subreddit')\n",
    "    print(f'{subreddit}.csv file has {df.shape[0]} records now')\n",
    "    subredits_total_recs.append(df.shape[0])\n",
    "\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "print(f'=====> total number of records collected so far <=====')\n",
    "[print(i) for i in zip(subreddits_to_check, subredits_total_recs)];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
