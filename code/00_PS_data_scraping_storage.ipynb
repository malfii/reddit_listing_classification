{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "---\n",
    "In this project we will be using Natural Language Processing (NLP) to classify different reddit listings into the subreddit (or read it as categories) they do belong to. Reddit is a network of communities where people can dive into their interests, hobbies and passions There's a community for whatever you're interested in where you can interact with people and get their opinion, or simply get your questions answered (source: reddit.com). To be able to interpret human language by machines, we need to put the language into a machine understandable format. NLP helps us do that. NLP helps computers communicate with humans in their own language and scales other language-related tasks. For example, NLP makes it possible for computers to read text, hear speech, interpret it, measure sentiment and determine which parts are important (source: sas.com). \n",
    "\n",
    "As easy as it sounds, the whole process of reading and interpreting the human language is not that straight forward. As part of the current project, we will try to use various tools like transformers and estimators to input text into models and interpret that so that we can answer the questions of 'whether it is possible to predict what subreddit a listing belongs to' and if 'our predictive model provides reliable forecasts?'. The term reliable here means what percentage of the predictions are correct when the model is used for unseen data (more on this later). This usually poses challenges as human language has complexities that would not be easy to capture with simple models. The problem is expressed in the context reading different postings related to different subjects in reddit and categorizing them based on some previously seen examples. The problem becomes specifically challenging when the classes were are trying to categorize are intrinsically related to each other (subjects that are not totally independent or irrelevant). \n",
    "\n",
    "Why is it important? Being able to classify a listing/post could help companies make informed decisions. For example, these methods could help us understand what the user is looking for and then we can provide more useful suggestions or a related user experience so that the user will get their needs met based on the type of request or interaction they have with the platform. The other applications of the current work includes, but is not limited to, spam classification, virtual assistants, translation and summarization, and urgency detection. \n",
    "\n",
    "Who benefits from this project? The primary beneficiaries of this work are companies who provide services to customers. Knowing what the customer is looking for would help the companies address customer needs and tailor solutions based on different requirements. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scraping \n",
    "---\n",
    "In this section, we will present the methods we have used to collect the data. The data for this project is scrapped using reddit API and the following steps were used:\n",
    "- Prepare API calls by using the user's credentials\n",
    "- Read the data from the subreddits of interest\n",
    "- Extract necessary data from the newly retrieved data\n",
    "- Look for the new listings in the data and add the new listings to the files that include the listings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os # to check for file existance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_listings(res, nskip = 0):\n",
    "    \"\"\"\n",
    "    This funciton accept the get object from an api request in reddit and extracts necessary information in the form of dataframe\n",
    "    Args:\n",
    "        res (request model response): request.get object from reddit \n",
    "        nskip (int): number of records to skip in each request (not necessary info), default it 0 for reddit\n",
    "    Return:\n",
    "        dataframe: a datafram with the following columns generated from the request ['text', 'title', 'listingid', 'created', 'url', 'media']\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    records = []\n",
    "    for listing in res.json()['data']['children']:\n",
    "        \n",
    "        # only use the information you need from the each listing  \n",
    "        text = listing['data']['selftext']\n",
    "        title = listing['data']['title']\n",
    "        listingid = listing['data']['id']\n",
    "        created = listing['data']['created']\n",
    "        url = listing['data']['url']\n",
    "        #media_embed = listing['data']['media_embed']\n",
    "        media = listing['data']['media']\n",
    "\n",
    "        records.append([text, title, listingid, created, url, media])\n",
    "\n",
    "    return pd.DataFrame(records, columns=['text', 'title', 'listingid', 'created', 'url', 'media'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scraping scripts\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first prepare the API calls based on the documentation from reddit and also our own credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read person information\n",
    "# Using readlines()\n",
    "file1 = open('./../personal/my_data.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    "\n",
    "# personal information in the file should be in the following format: \n",
    "'''\n",
    "client_id =?\n",
    "client_secret =?\n",
    "user_agent =?\n",
    "username =?\n",
    "password =?\n",
    "'''\n",
    "\n",
    "# Strips the newline character\n",
    "personal = []\n",
    "for line in Lines:\n",
    "    personal.append(line.split('=')[1].replace('\\n', ''))\n",
    "\n",
    "client_id = personal[0] #alphanumeric string provided under \"personal use script\"\n",
    "client_secret = personal[1] #alphanumeric string provided as \"secret\"\n",
    "user_agent = personal[2] #the name of your application\n",
    "username =  personal[3] #your reddit username\n",
    "password =  personal[4] #your reddit password\n",
    "\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use basic authentication framework\n",
    "auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': username,\n",
    "    'password': password\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#create an informative header for your application\n",
    "headers = {'User-Agent': 'massiproj3/0.0.1'}\n",
    "\n",
    "res = requests.post(\n",
    "    'https://www.reddit.com/api/v1/access_token',\n",
    "    auth=auth,\n",
    "    data=data,\n",
    "    headers=headers)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve access token\n",
    "token = res.json()['access_token']\n",
    "# add access token to the header file\n",
    "headers['Authorization'] = f'bearer {token}'\n",
    "\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers).status_code == 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block of the code, we will take a list of the subreddits we are interested in. If there is no previous information collected from those subreddits, we will create a new .csv file and start collecting the data. If we already have some data from that subreddit, it will just append the new data to what we already have. It is recommended that we run this block of the code every 2-3 days so that new information will be fetched and recorded. \n",
    "Subreddits with similarities (based on users posting in subreddits, source: https://anvaka.github.io/sayit/?query=dating_advice):\n",
    "- offmychest, askreddit, nostupidquestions, dating_advice, relationship_advice\n",
    "- legaladvice, nostupidquestions\n",
    "- dating_advice, relationship_advice \n",
    "- investing, wallstreetbets (just titles, no text as a lot of pics exist)\n",
    "- politics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since reddit has a limit of 1000 API calls per minute, we limitted the number of subreddits for data extraction to 10 a and number of calls to 9 per subreddit so that we will not go above the limit and based on the observations from the number of listings per day for the subreddits, this should be enough for this project. If more data needs to be collected, the user should consider to set up some autopause during the scraping process so the number of calls does not go above 1000 per min**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++> no initial data extraction for subreddit offmychest as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit trueoffmychest as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit askreddit as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit nostupidquestions as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit dating_advice as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit relationship_advice as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit legaladvice as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit politics as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit investing as related .csv file exists in the folder\n",
      "++> no initial data extraction for subreddit wallstreetbets as related .csv file exists in the folder\n",
      "==> extracting daily information for subreddit offmychest\n",
      "request.get() status for the 1\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 2\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 3\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 4\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 5\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 6\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 7\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 8\"s call for subreddit offmychest is 200\n",
      "request.get() status for the 9\"s call for subreddit offmychest is 200\n",
      "a total of 900 new listings was added to offmychest subreddit\n",
      "offmychest.csv file has 7120 records now\n",
      "==> extracting daily information for subreddit trueoffmychest\n",
      "request.get() status for the 1\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 2\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 3\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 4\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 5\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 6\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 7\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 8\"s call for subreddit trueoffmychest is 200\n",
      "request.get() status for the 9\"s call for subreddit trueoffmychest is 200\n",
      "a total of 827 new listings was added to trueoffmychest subreddit\n",
      "trueoffmychest.csv file has 4996 records now\n",
      "==> extracting daily information for subreddit askreddit\n",
      "request.get() status for the 1\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 2\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 3\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 4\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 5\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 6\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 7\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 8\"s call for subreddit askreddit is 200\n",
      "request.get() status for the 9\"s call for subreddit askreddit is 200\n",
      "a total of 900 new listings was added to askreddit subreddit\n",
      "askreddit.csv file has 13629 records now\n",
      "==> extracting daily information for subreddit nostupidquestions\n",
      "request.get() status for the 1\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 2\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 3\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 4\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 5\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 6\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 7\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 8\"s call for subreddit nostupidquestions is 200\n",
      "request.get() status for the 9\"s call for subreddit nostupidquestions is 200\n",
      "a total of 900 new listings was added to nostupidquestions subreddit\n",
      "nostupidquestions.csv file has 11795 records now\n",
      "==> extracting daily information for subreddit dating_advice\n",
      "request.get() status for the 1\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 2\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 3\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 4\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 5\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 6\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 7\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 8\"s call for subreddit dating_advice is 200\n",
      "request.get() status for the 9\"s call for subreddit dating_advice is 200\n",
      "a total of 900 new listings was added to dating_advice subreddit\n",
      "dating_advice.csv file has 5610 records now\n",
      "==> extracting daily information for subreddit relationship_advice\n",
      "request.get() status for the 1\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 2\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 3\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 4\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 5\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 6\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 7\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 8\"s call for subreddit relationship_advice is 200\n",
      "request.get() status for the 9\"s call for subreddit relationship_advice is 200\n",
      "a total of 900 new listings was added to relationship_advice subreddit\n",
      "relationship_advice.csv file has 9141 records now\n",
      "==> extracting daily information for subreddit legaladvice\n",
      "request.get() status for the 1\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 2\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 3\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 4\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 5\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 6\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 7\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 8\"s call for subreddit legaladvice is 200\n",
      "request.get() status for the 9\"s call for subreddit legaladvice is 200\n",
      "a total of 900 new listings was added to legaladvice subreddit\n",
      "legaladvice.csv file has 6340 records now\n",
      "==> extracting daily information for subreddit politics\n",
      "request.get() status for the 1\"s call for subreddit politics is 200\n",
      "request.get() status for the 2\"s call for subreddit politics is 200\n",
      "request.get() status for the 3\"s call for subreddit politics is 200\n",
      "request.get() status for the 4\"s call for subreddit politics is 200\n",
      "request.get() status for the 5\"s call for subreddit politics is 200\n",
      "request.get() status for the 6\"s call for subreddit politics is 200\n",
      "request.get() status for the 7\"s call for subreddit politics is 200\n",
      "request.get() status for the 8\"s call for subreddit politics is 200\n",
      "request.get() status for the 9\"s call for subreddit politics is 200\n",
      "a total of 481 new listings was added to politics subreddit\n",
      "politics.csv file has 2553 records now\n",
      "==> extracting daily information for subreddit investing\n",
      "request.get() status for the 1\"s call for subreddit investing is 200\n",
      "request.get() status for the 2\"s call for subreddit investing is 200\n",
      "request.get() status for the 3\"s call for subreddit investing is 200\n",
      "request.get() status for the 4\"s call for subreddit investing is 200\n",
      "request.get() status for the 5\"s call for subreddit investing is 200\n",
      "request.get() status for the 6\"s call for subreddit investing is 200\n",
      "request.get() status for the 7\"s call for subreddit investing is 200\n",
      "request.get() status for the 8\"s call for subreddit investing is 200\n",
      "request.get() status for the 9\"s call for subreddit investing is 200\n",
      "a total of 136 new listings was added to investing subreddit\n",
      "investing.csv file has 1429 records now\n",
      "==> extracting daily information for subreddit wallstreetbets\n",
      "request.get() status for the 1\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 2\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 3\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 4\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 5\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 6\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 7\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 8\"s call for subreddit wallstreetbets is 200\n",
      "request.get() status for the 9\"s call for subreddit wallstreetbets is 200\n",
      "a total of 162 new listings was added to wallstreetbets subreddit\n",
      "wallstreetbets.csv file has 1618 records now\n",
      "=====> total number of records collected so far <=====\n",
      "('offmychest', 7120)\n",
      "('trueoffmychest', 4996)\n",
      "('askreddit', 13629)\n",
      "('nostupidquestions', 11795)\n",
      "('dating_advice', 5610)\n",
      "('relationship_advice', 9141)\n",
      "('legaladvice', 6340)\n",
      "('politics', 2553)\n",
      "('investing', 1429)\n",
      "('wallstreetbets', 1618)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "subreddits_to_check = ['offmychest', 'trueoffmychest', 'askreddit', 'nostupidquestions', \n",
    "                       'dating_advice', 'relationship_advice',\n",
    "                       'legaladvice', \n",
    "                       'politics', \n",
    "                       'investing', 'wallstreetbets']\n",
    "subredits_total_recs = []\n",
    "req_per_day = 9 # number of inqueries for each subreddit every time we run this script\n",
    "\n",
    "# This section collects the 1000 available records for a given subreddit\n",
    "# Note that this block of code will run just one time to initially retrive 1000 records \n",
    "# for each of the subreddits in the list and create a .csv file for it. \n",
    "# this does not run for subreddits that already have a .csv file associated to them. instead, the \n",
    "# next block of the code will run for those. \n",
    "\n",
    "for subreddit in subreddits_to_check:\n",
    "    path = './../dataset/'+ subreddit+'.csv'\n",
    "    count = 0\n",
    "    if not os.path.isfile(path):    # continue only if the file does not exist\n",
    "        print(f'++> extracting initial information for subreddit {subreddit}')\n",
    "        # read the first 100\n",
    "        url = 'https://oauth.reddit.com/r/'+subreddit+'/new'\n",
    "        params = {\n",
    "            'limit': 100\n",
    "        # 'after': <-- will be important for getting the 'next' posts\n",
    "        }\n",
    "        res = requests.get(url, \n",
    "                        headers=headers,\n",
    "                        params=params)\n",
    "        count += 1\n",
    "        print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "        bottom_of_listing = res.json()['data']['after']\n",
    "        df = extract_listings(res, nskip = 0)\n",
    "        # continue reading the rest of 900 in batches of 100\n",
    "        for i in range(9):\n",
    "            params = {'limit': 100, \n",
    "                    'after': bottom_of_listing   # this will read 100 records under the first one\n",
    "                    }\n",
    "            res = requests.get(url, \n",
    "                            headers=headers,\n",
    "                            params=params) \n",
    "            count += 1\n",
    "            print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "            bottom_of_listing = res.json()['data']['after']\n",
    "            df = pd.concat([df, extract_listings(res, nskip = 0)], axis=0) \n",
    "        \n",
    "        print(f'df shape before dropping the repeats {df.shape}')\n",
    "        df = df.drop_duplicates(subset='listingid', keep='first')\n",
    "        print(f'df shape after dropping the repeats {df.shape}')\n",
    "        df.to_csv(path, index=False)\n",
    "    else:\n",
    "        print(f'++> no initial data extraction for subreddit {subreddit} as related .csv file exists in the folder')\n",
    "\n",
    "\n",
    "# this section of the code is intended to run every2-3 days to retrieve new information for the given list of \n",
    "# the subreddits of interest. the new information will be appended to the list of the .csv files we currently have \n",
    "# and will be saved into the appropriate .csv file\n",
    "        \n",
    "for subreddit in subreddits_to_check:   \n",
    "    path = './../dataset/'+ subreddit+'.csv'\n",
    "    count = 0 \n",
    "    df = pd.read_csv(path) # get the current information we have\n",
    "\n",
    "    print(f'==> extracting daily information for subreddit {subreddit}')\n",
    "    # read the first 100\n",
    "    url = 'https://oauth.reddit.com/r/'+subreddit+'/new'\n",
    "    params = {\n",
    "        'limit': 100\n",
    "    # 'after': <-- will be important for getting the 'next' posts\n",
    "    }\n",
    "    res = requests.get(url, \n",
    "                       headers=headers,\n",
    "                       params=params)\n",
    "    count += 1\n",
    "    print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "    bottom_of_listing = res.json()['data']['after']\n",
    "    temp = extract_listings(res, nskip = 0)\n",
    "    # continue reading the rest of 900 in batches of 100\n",
    "    for i in range(req_per_day-1):\n",
    "        params = {'limit': 100, \n",
    "                  'after': bottom_of_listing   # this will read 100 records under the first one\n",
    "                  }\n",
    "        res = requests.get(url, \n",
    "                        headers=headers,\n",
    "                        params=params) \n",
    "        count += 1\n",
    "        print(f'request.get() status for the {count}\"s call for subreddit {subreddit} is {res.status_code}')\n",
    "        bottom_of_listing = res.json()['data']['after']\n",
    "        temp = pd.concat([temp, extract_listings(res, nskip = 0)], axis=0) \n",
    "    temp = temp.drop_duplicates(subset='listingid', keep='first')\n",
    "    old_size = df.shape[0]\n",
    "    df = pd.concat([df, temp], axis=0) \n",
    "    df = df.drop_duplicates(subset='listingid', keep='first')\n",
    "\n",
    "    print(f'a total of {df.shape[0]-old_size} new listings was added to {subreddit} subreddit')\n",
    "    print(f'{subreddit}.csv file has {df.shape[0]} records now')\n",
    "    subredits_total_recs.append(df.shape[0])\n",
    "\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "print(f'=====> total number of records collected so far <=====')\n",
    "[print(i) for i in zip(subreddits_to_check, subredits_total_recs)];\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
