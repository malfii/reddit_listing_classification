{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production model and conclusions\n",
    "In this notebook, we will pick up our best model from the model tuning notebook and look at its coefficients. Also, we will interpret how this model performs in the case of imbalance data, and how we might want to modify it for data imbalance. <br>\n",
    "At the end, we will be diving into project conclusions and future recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "import re \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # for sentiment analyzer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and train/test split\n",
    "This part is similar to the previous notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from preprocessed data that are saved into files\n",
    "df = pd.read_csv('./../dataset/offmychestrelationship_advice_processed.csv')\n",
    "\n",
    "X = df[['text', 'word_count', 'sentiment']]\n",
    "y = df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y)\n",
    "X_train = pd.DataFrame(X_train, columns=['text', 'word_count', 'sentiment'])\n",
    "X_test = pd.DataFrame(X_test, columns=['text', 'word_count', 'sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production model\n",
    "\n",
    "This is the best performing model from the tuning stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\masou\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;cvec&#x27;,\n",
       "                 CountVectorizer(max_features=3000, min_df=4,\n",
       "                                 ngram_range=(1, 2), stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;logr&#x27;, LogisticRegression(C=0.07))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;cvec&#x27;,\n",
       "                 CountVectorizer(max_features=3000, min_df=4,\n",
       "                                 ngram_range=(1, 2), stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;logr&#x27;, LogisticRegression(C=0.07))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=3000, min_df=4, ngram_range=(1, 2),\n",
       "                stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.07)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_features=3000, min_df=4,\n",
       "                                 ngram_range=(1, 2), stop_words='english')),\n",
       "                ('logr', LogisticRegression(C=0.07))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create X train and test best on only the text column\n",
    "Z_train = X_train['text']\n",
    "Z_test = X_test['text']\n",
    "\n",
    "# use the best model from tuning notebook and refit\n",
    "pipe_cvec_logr_best = Pipeline([\n",
    "    ('cvec', CountVectorizer(max_df=1.0,\n",
    "                             max_features=3000,\n",
    "                             min_df=4,\n",
    "                             ngram_range=(1,2),\n",
    "                             stop_words='english')),\n",
    "    ('logr', LogisticRegression(C=0.07,\n",
    "                                penalty='l2'))\n",
    "])\n",
    "pipe_cvec_logr_best.fit(Z_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9497232472324724\n",
      "test score: 0.8865662465416538\n"
     ]
    }
   ],
   "source": [
    "print(f'train score: {pipe_cvec_logr_best.score(Z_train, y_train)}')\n",
    "print(f'test score: {pipe_cvec_logr_best.score(Z_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.56225\n",
       "0    0.43775\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data seems quite balance. Let us extract some precision, recall, and f1 scores as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.9\n",
      "recall: 0.9\n",
      "f1-score:0.9\n"
     ]
    }
   ],
   "source": [
    "pred = pipe_cvec_logr_best.predict(Z_test)\n",
    "prec, recall, f1score, _ = classification_report(y_test, pred, output_dict=True)['1'].values()\n",
    "print(f'precision: {round(prec,2)}' \n",
    "      f'\\nrecall: {round(recall, 2)}'\n",
    "      f'\\nf1-score:{round(f1score, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential analysis on LogReg\n",
    "Since we have been using logistic regression, we should be able to interpret the model and coefficients. In logistic regression, each coefficient shows the change in log odds of the event happening when the value of the feature changes by one unit. In the context of this project, the features are the number of times the popular words have appeared in our training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us look at the 10 words with the lowest coefficients in the logistic regression. The negative coefficient means as the number of these coefficients increases, the odds of the posts being classified as 0 (r/offmychest) increases. Since these coefficients have the biggest absolute values, a unit increase in the number of these words in a post impacts the log(odd) of our classification by the value of the coefficient (in favor of r/offmychest or the 0 class). For example, if the number of 'guy friends' increase in a post by one unit, the odds of that post being classified as a positive class (r/relationship_advice) decreases exp(-0.86) times, or becomes half. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>guy friends</td>\n",
       "      <td>-0.864334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>entire</td>\n",
       "      <td>-0.759745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>time job</td>\n",
       "      <td>-0.451939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>sending</td>\n",
       "      <td>-0.451759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>arrived</td>\n",
       "      <td>-0.450660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>using</td>\n",
       "      <td>-0.444252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>unhealthy</td>\n",
       "      <td>-0.423991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>know</td>\n",
       "      <td>-0.404741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>started crying</td>\n",
       "      <td>-0.403140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>blow</td>\n",
       "      <td>-0.400930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>crap</td>\n",
       "      <td>-0.400097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>know like</td>\n",
       "      <td>-0.393268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>f24</td>\n",
       "      <td>-0.392359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>time feel</td>\n",
       "      <td>-0.383291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>power</td>\n",
       "      <td>-0.375113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>swear</td>\n",
       "      <td>-0.370307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2827</th>\n",
       "      <td>advantage</td>\n",
       "      <td>-0.369452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>27</td>\n",
       "      <td>-0.369203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>affair</td>\n",
       "      <td>-0.368664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>right away</td>\n",
       "      <td>-0.362337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>difference</td>\n",
       "      <td>-0.359889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>disgusting</td>\n",
       "      <td>-0.355790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>properly</td>\n",
       "      <td>-0.352228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>just don</td>\n",
       "      <td>-0.349105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>got know</td>\n",
       "      <td>-0.348489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>25</td>\n",
       "      <td>-0.345111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>views</td>\n",
       "      <td>-0.342757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>atleast</td>\n",
       "      <td>-0.338379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>mentally</td>\n",
       "      <td>-0.337950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>helpful</td>\n",
       "      <td>-0.335378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             feature  coefficient\n",
       "2816     guy friends    -0.864334\n",
       "442           entire    -0.759745\n",
       "2932        time job    -0.451939\n",
       "1731         sending    -0.451759\n",
       "1416         arrived    -0.450660\n",
       "1235           using    -0.444252\n",
       "2080       unhealthy    -0.423991\n",
       "10              know    -0.404741\n",
       "1431  started crying    -0.403140\n",
       "1564            blow    -0.400930\n",
       "2081            crap    -0.400097\n",
       "1077       know like    -0.393268\n",
       "208              f24    -0.392359\n",
       "1443       time feel    -0.383291\n",
       "1120           power    -0.375113\n",
       "381            swear    -0.370307\n",
       "2827       advantage    -0.369452\n",
       "1354              27    -0.369203\n",
       "9             affair    -0.368664\n",
       "946       right away    -0.362337\n",
       "2393      difference    -0.359889\n",
       "1508      disgusting    -0.355790\n",
       "673         properly    -0.352228\n",
       "218         just don    -0.349105\n",
       "2874        got know    -0.348489\n",
       "1282              25    -0.345111\n",
       "1212           views    -0.342757\n",
       "2232         atleast    -0.338379\n",
       "1075        mentally    -0.337950\n",
       "2525         helpful    -0.335378"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs_df = pd.DataFrame(list(zip(pipe_cvec_logr_best.named_steps['cvec'].vocabulary_.keys(),\n",
    "                                  pipe_cvec_logr_best.named_steps['logr'].coef_[0][:])), columns=['feature', 'coefficient'])\n",
    "coeffs_df.sort_values(by='coefficient', ascending=True).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us do similar analysis for words with the highest (and positive) coefficients in our logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>speaking</td>\n",
       "      <td>0.828800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>remember</td>\n",
       "      <td>0.833967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>offended</td>\n",
       "      <td>0.844299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>breaks</td>\n",
       "      <td>0.847692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>away</td>\n",
       "      <td>0.860259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>wait</td>\n",
       "      <td>0.862766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>couch</td>\n",
       "      <td>0.864325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>touch</td>\n",
       "      <td>0.869269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>don</td>\n",
       "      <td>0.871016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>care</td>\n",
       "      <td>0.877309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>starts</td>\n",
       "      <td>0.881947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>went</td>\n",
       "      <td>0.887026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>bit</td>\n",
       "      <td>0.894479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>states</td>\n",
       "      <td>0.903913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>asleep</td>\n",
       "      <td>0.928496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>majority</td>\n",
       "      <td>0.939378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>sleep</td>\n",
       "      <td>0.947467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>couldn</td>\n",
       "      <td>0.962737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>woke</td>\n",
       "      <td>0.968992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>person</td>\n",
       "      <td>0.989151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>working</td>\n",
       "      <td>0.996305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>apologized</td>\n",
       "      <td>1.000667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>favorite</td>\n",
       "      <td>1.015481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>god</td>\n",
       "      <td>1.018417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>turn</td>\n",
       "      <td>1.036739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>trust</td>\n",
       "      <td>1.084197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>normally</td>\n",
       "      <td>1.157377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>change</td>\n",
       "      <td>1.158203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>light</td>\n",
       "      <td>1.205888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>got</td>\n",
       "      <td>1.264831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature  coefficient\n",
       "13      speaking     0.828800\n",
       "54      remember     0.833967\n",
       "921     offended     0.844299\n",
       "72        breaks     0.847692\n",
       "16          away     0.860259\n",
       "66          wait     0.862766\n",
       "67         couch     0.864325\n",
       "925        touch     0.869269\n",
       "27           don     0.871016\n",
       "59          care     0.877309\n",
       "55        starts     0.881947\n",
       "48          went     0.887026\n",
       "46           bit     0.894479\n",
       "15        states     0.903913\n",
       "40        asleep     0.928496\n",
       "2669    majority     0.939378\n",
       "49         sleep     0.947467\n",
       "51        couldn     0.962737\n",
       "45          woke     0.968992\n",
       "12        person     0.989151\n",
       "39       working     0.996305\n",
       "923   apologized     1.000667\n",
       "919     favorite     1.015481\n",
       "60           god     1.018417\n",
       "36          turn     1.036739\n",
       "30         trust     1.084197\n",
       "26      normally     1.157377\n",
       "37        change     1.158203\n",
       "42         light     1.205888\n",
       "33           got     1.264831"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs_df.sort_values(by='coefficient', ascending=True).tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the list above, we can say that presence (or increase in the number of times these words are used) of the words like 'trust', 'god', 'apologized', 'favorite' has the highest impact on increasing the log(odd) of a listing being classified as 1 (r/relationship_advice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production model with imbalance data\n",
    "So far, we have been using balanced data. Now, let us assume the data is imbalance (minority class is '1' that is somehow important for us as well-e.g. for this project, class 1 is r/relationship_advice data and let us say we did not have that much of info from the relationship_advice subreddit but being able to classify this subreddit was very important for us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df['subreddit'] == 1]\n",
    "temp = temp.sample(frac=0.06)\n",
    "df_imbalance = pd.concat([df[df['subreddit'] == 0], temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.928534\n",
       "1    0.071466\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imbalance['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the df_imbalance dataframe is severely imbalanced now and the 1 category (which we are 'assuming' is important for us) is underrepresented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ximb = df_imbalance.drop(columns='subreddit')\n",
    "yimb = df_imbalance['subreddit']\n",
    "Ximb_train, Ximb_test, yimb_train, yimb_test = train_test_split(Ximb, yimb,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=42,\n",
    "                                                                stratify=yimb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07170795306388526, 0.07140528203456147)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yimb_test.mean(), yimb_train.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;cvec&#x27;,\n",
       "                 CountVectorizer(max_features=3000, min_df=4,\n",
       "                                 ngram_range=(1, 2), stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;logr&#x27;, LogisticRegression(C=0.07))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;cvec&#x27;,\n",
       "                 CountVectorizer(max_features=3000, min_df=4,\n",
       "                                 ngram_range=(1, 2), stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;logr&#x27;, LogisticRegression(C=0.07))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_features=3000, min_df=4, ngram_range=(1, 2),\n",
       "                stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.07)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(max_features=3000, min_df=4,\n",
       "                                 ngram_range=(1, 2), stop_words='english')),\n",
       "                ('logr', LogisticRegression(C=0.07))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Z train and test based on only the text column\n",
    "Zimb_train = Ximb_train['text']\n",
    "Zimb_test = Ximb_test['text']\n",
    "\n",
    "# fit the previous model to this new data\n",
    "pipe_cvec_logr_best.fit(Zimb_train, yimb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9747310074991848\n",
      "test score: 0.9282920469361148\n"
     ]
    }
   ],
   "source": [
    "# see the scores\n",
    "print(f'train score: {pipe_cvec_logr_best.score(Zimb_train, yimb_train)}')\n",
    "print(f'test score: {pipe_cvec_logr_best.score(Zimb_test, yimb_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the score (accuracy) is actually very good. In fact, the accuracy of the model is even better than when we did not have imbalance data. This is because in the case of data imbalance, the model becomes impacted by the majority class and while it misclassifies a big portion of that minority class, it's overall performance (accuracy) becomes better because it is easier to just focus of the majority class and predict those. <br> \n",
    "Let us see how our model has been performing on our minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.5\n",
      "recall: 0.28\n",
      "f1-score:0.36\n"
     ]
    }
   ],
   "source": [
    "predimb = pipe_cvec_logr_best.predict(Zimb_test)\n",
    "prec_imb, recall_imb, f1score_imb, _ = classification_report(yimb_test, predimb, output_dict=True)['1'].values()\n",
    "print(f'precision: {round(prec_imb,2)}' \n",
    "      f'\\nrecall: {round(recall_imb, 2)}'\n",
    "      f'\\nf1-score:{round(f1score_imb, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the original production model performs very poorly on the minority class when the data show strong imbalance because the majority will overtake the training process. Let us see if we can alleviate this issue. To do that, we will retrain the model by giving a higher weight to minority class (using `class_weight`). This might reduce our overall model score but should help us get better metrics on the minority class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\masou\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9791327029670688\n",
      "test score: 0.9061277705345502\n",
      "precision: 0.4\n",
      "recall: 0.6\n",
      "f1-score:0.48\n"
     ]
    }
   ],
   "source": [
    "# use the best model from tuning notebook and refit\n",
    "pipe_cvec_logr_imb = Pipeline([\n",
    "    ('cvec', CountVectorizer(max_df=1.0,\n",
    "                             max_features=3000,\n",
    "                             min_df=4,\n",
    "                             ngram_range=(1,2),\n",
    "                             stop_words='english')),\n",
    "    ('logr', LogisticRegression(C=0.07,\n",
    "                                penalty='l2',\n",
    "                                class_weight={0:1, 1:30}))\n",
    "])\n",
    "pipe_cvec_logr_imb.fit(Zimb_train, yimb_train)\n",
    "\n",
    "# see the scores\n",
    "print(f'train score: {pipe_cvec_logr_imb.score(Zimb_train, yimb_train)}')\n",
    "print(f'test score: {pipe_cvec_logr_imb.score(Zimb_test, yimb_test)}')\n",
    "predimb = pipe_cvec_logr_imb.predict(Zimb_test)\n",
    "prec_imb, recall_imb, f1score_imb, _ = classification_report(yimb_test, predimb, output_dict=True)['1'].values()\n",
    "print(f'precision: {round(prec_imb,2)}' \n",
    "      f'\\nrecall: {round(recall_imb, 2)}'\n",
    "      f'\\nf1-score:{round(f1score_imb, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows model performance for the original production model vs. that of weighted model for severely imbalanced data:\n",
    "\n",
    "| metric | model without weighted classes | model with weighted classes|\n",
    "| --------      | ------ | ------|\n",
    "| test score    | 0.93   | 0.91  |\n",
    "| precision     | 0.50  | 0.40  |\n",
    "| recall        | 0.28   | 0.60  |\n",
    "| f1-score      | 0.36   | 0.48  |\n",
    "\n",
    "As we can see from these scores, giving more weight to the positive class (the minority class), our model is able to predict more positives (and a higher percentage of actual positives), this means the model sensitivity (recall) increases and at the same time, the number of false positives increases as well, which means a decreased precision. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key takeaways\n",
    "\n",
    "As we have seen, the production model does not have any issue predicting balanced data, however, when the data become imbalance, then same production data struggles to predict the positive class (the imbalance class). For this to be resolved, we had to modify the production model by giving more weight to the underrepresented class (here the positive class) so that the model can perform better on that class (and predict more positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project conclusions and recommendations\n",
    "In this project, we used the reddit API's to scape more than 15,000 listings on different subreddits (r/offmychest and r/relationship_advice) and analyze their information. The primary goal of this work is to see whether we can use NLP to classify different listings in reddit based on the subreddit they belong to. To achieve this goal we have employed multiple estimators (KNN, Logistic Regression, Naive Bayes, and Random Forest) along with different transformers (CountVectorizer and TfidfVectorizer). Here are a few important findings:\n",
    "- EDA has revealed that there are groups of popular words that are unique to each of these subreddits and could be used as indicators for our classification problems. \n",
    "- In addition to the text in each listing, we have discovered different pattern in the data (e.g. sentiment analysis and length of the listing) that could be used to gain more information for our classification problem. \n",
    "- Among all the estimators and transformers, we have found that CountVectorizer with Logistic regression generates us the best results. \n",
    "- A general trend that was observed in all of the classifiers was the fact that our models were all having a relatively high variance. Increasing the number of words we used for our classification usually improved the training score without that much of help with the test data (overfitting). \n",
    "- Since neither of the subreddits we were investigating was more important than the other one, we used the accuracy score (tn + tp)/total observations in most of our evaluations. \n",
    "- The original data were were using was quite balance but to see how the production model performs on imbalanced data, we created some severely imbalanced (while the positive class was of more importance for us). The base model was not particularly the best model for the imbalance data so what we did was to increase the weight of the underrepresented class in our model and use metrics like precision, recall, and f1-score instead so that we tune the model for the specific purpose of interest, and the class of interest. Using this approach, we were able to improve model's performance for underrepresented classes. <br>\n",
    "These findings are important in a sense that these results could be used to classify subreddit listings with an accuracy of 88%. In addition to that, we have suggested some extra steps that help with imbalance data and how we can modify the modeling process to get the best metrics (depending on whether outcomes like false positive or false negative are important for us) based on the scope and purpose of the research. \n",
    "\n",
    "**Future recommendations**\n",
    "\n",
    "Here are a few areas identified as where we could further work and improve upon:\n",
    "- Automate data scraping by using timed scripts.\n",
    "- Use features other than just the text of the listing. Information like location, time, etc might help us improve performance of our classifier. This might require collecting data consistently, over a longer period of time. \n",
    "- The current transformers do not consider the relation between the words and the context in which the words are used. Instead, using more advanced language models to capture the meaning and relation between the words and having more targeted data preparation could help us get better results. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
